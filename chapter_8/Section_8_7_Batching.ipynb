{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Section 8.7 Batching\n",
        "\n",
        "In this code, we demonstrate how to utilize the NeighborLoader class for efficient\n",
        "batch processing of graph data. Additionally, we will track the system's performance\n",
        "including memory usage and time efficiency using libraries like pynvml, thop, and time.\n",
        "\n",
        "This code is organized into four major parts:\n",
        "\n",
        "\n",
        "1.   Install and import the required packages and datasets\n",
        "2.   Data preparation and loading\n",
        "3.   Model setup\n",
        "4.   Model training with performance tracking\n",
        "\n",
        "-------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "4GGiVabURk0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I. Install Packages, Load Data and Import Packages\n"
      ],
      "metadata": {
        "id": "U5bh5DQ4V6oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the CUDA version PyTorch was installed with\n",
        "!python -c \"import torch; print(torch.version.cuda)\""
      ],
      "metadata": {
        "id": "F6yxbxHdJSAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch version\n",
        "!python -c \"import torch; print(torch.__version__)\""
      ],
      "metadata": {
        "id": "9MYYJ9nnJThR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the above information to fill in the http address below\n",
        "# %%capture\n",
        "!pip install ogb pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "DMViVQi5DToJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gputil\n",
        "!pip install nvidia-ml-py3\n",
        "!pip install thop"
      ],
      "metadata": {
        "id": "z_ZrcJ4VdXfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import collections\n",
        "import gc\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Third-party library imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import psutil\n",
        "import pynvml\n",
        "from thop import clever_format, profile\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.loader import NeighborLoader, NeighborSampler\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import os.path as osp\n",
        "\n"
      ],
      "metadata": {
        "id": "4Ss3zaZ_ARlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is dedicated to preparing and processing the dataset. It loads the Amazon products dataset, creates training, validation, and test masks, and sets up NeighborLoaders to load batches of graph data during training and evaluation.\n"
      ],
      "metadata": {
        "id": "6zo4MFdAd5VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download and loading the obg dataset\n",
        "root = osp.join(osp.dirname(osp.realpath('./')), 'data', 'products')\n",
        "# dataset_sparse = PygNodePropPredDataset( name='ogbn-products', transform=T.ToSparseTensor())"
      ],
      "metadata": {
        "id": "uBe3sdKEQJ--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dense = PygNodePropPredDataset( name='ogbn-products')\n",
        "dataset_dense.num_classes"
      ],
      "metadata": {
        "id": "GHvCJd_41HLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code snippet:\n",
        "\n",
        "1.  The dataset is split into training, testing, and validation subsets.\n",
        "2.  Boolean masks are created for each subset, which allows for easy and efficient access to these subsets during the model training and evaluation stages.\n",
        "3.  These masks are then assigned to the data_dense object, enhancing the ease of accessing and manipulating the various data subsets."
      ],
      "metadata": {
        "id": "OiZUkKXWetKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the first element from the dataset in dense format\n",
        "data_dense = dataset_dense[0]\n",
        "\n",
        "# Splitting the dataset into training, validation, and test sets\n",
        "split_idx = dataset_dense.get_idx_split()\n",
        "\n",
        "# Extracting indices for training, validation, and test sets\n",
        "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "\n",
        "# Creating boolean masks for easy data segmentation\n",
        "train_mask = torch.zeros(data_dense.num_nodes, dtype=torch.bool)\n",
        "train_mask[train_idx] = True\n",
        "\n",
        "test_mask = torch.zeros(data_dense.num_nodes, dtype=torch.bool)\n",
        "test_mask[test_idx] = True\n",
        "\n",
        "valid_mask = torch.zeros(data_dense.num_nodes, dtype=torch.bool)\n",
        "valid_mask[valid_idx] = True\n",
        "\n",
        "# Assigning masks to the dense data object for easy access to data subsets\n",
        "data_dense.train_mask = train_mask\n",
        "data_dense.test_mask = test_mask\n",
        "data_dense.valid_mask = valid_mask\n"
      ],
      "metadata": {
        "id": "wwMmlNBQm1LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating NeighborLoaders\n",
        "# The `train_loader` is optimized for training speed with multiple neighbors and large batch size.\n",
        "# The `valid_loader` considers all neighbors for accuracy in the model validation phase.\n",
        "# The `test_loader` evaluates the model's final performance on unseen data with a broad neighborhood context.\n",
        "\n",
        "loader = NeighborLoader(\n",
        "    data_dense, num_neighbors=[20, 15, 10], batch_size=1280*5,\n",
        "    input_nodes=train_idx, shuffle=True, num_workers=6\n",
        ")\n",
        "\n",
        "valid_loader = NeighborLoader(\n",
        "    data_dense, num_neighbors=[-1], batch_size=256,\n",
        "    input_nodes=valid_idx, num_workers=6\n",
        ")\n",
        "\n",
        "test_loader = NeighborLoader(\n",
        "    data_dense, num_neighbors=[50] * 2, batch_size=256,\n",
        "    input_nodes=test_idx, num_workers=6\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "NokKtVxRUy6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the OGB evaluator for the dataset\n",
        "evaluator = Evaluator(name='ogbn-products')\n",
        "\n",
        "# Establish the device for model training 'cuda' if GPU, 'cpu' otherwise\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device)\n",
        "\n",
        "# Confirm the device. If it's a GPU, 'cuda' will print\n",
        "print('Device: {}'.format(device))"
      ],
      "metadata": {
        "id": "Ocpu1O6hj5An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III. Model Setup\n",
        "\n",
        "In this part, the GraphSAGE model is defined. It's a PyTorch module consisting of several GraphSAGE convolutional layers followed by activation and dropout operations. The model is meant to be trained on the node property prediction task."
      ],
      "metadata": {
        "id": "rK_KP119RDUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the GraphSAGE model\n",
        "class GraphSAGE_dense(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
        "        \"\"\"\n",
        "        Initialize the GraphSAGE_dense model.\n",
        "\n",
        "        Parameters:\n",
        "        - in_dim (int): The size of the input feature dimension.\n",
        "        - hidden_dim (int): The size of the hidden layer dimension.\n",
        "        - out_dim (int): The size of the output layer dimension.\n",
        "        - dropout (float): The dropout rate for regularization, default is 0.2.\n",
        "        \"\"\"\n",
        "\n",
        "        # Call the constructor of the parent class (torch.nn.Module)\n",
        "        super(GraphSAGE_dense, self).__init__()\n",
        "\n",
        "        # Store dropout rate\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Create the first GraphSAGE convolution layer\n",
        "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
        "\n",
        "        # Create the second GraphSAGE convolution layer\n",
        "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Create the third GraphSAGE convolution layer\n",
        "        self.conv3 = SAGEConv(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        Forward propagation of the model.\n",
        "\n",
        "        Parameters:\n",
        "        - x (torch.Tensor): The input features of nodes.\n",
        "        - edge_index (torch.Tensor): The edge indices.\n",
        "\n",
        "        Returns:\n",
        "        - x (torch.Tensor): The output features of nodes.\n",
        "        \"\"\"\n",
        "\n",
        "        # First GraphSAGE layer followed by an Exponential Linear Unit (ELU) activation and dropout\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        # Second GraphSAGE layer followed by an ELU activation and dropout\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        # Third GraphSAGE layer to produce the output\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        return x  # Return the output features of nodes\n",
        "\n"
      ],
      "metadata": {
        "id": "CxW10Vi4d8WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part IV. Model Training\n",
        "\n",
        "For the exploratory data analysis, we will:\n",
        "\n",
        "\n",
        "1.   Train GraphSage\n",
        "2.   Train GCN\n"
      ],
      "metadata": {
        "id": "eSPdpwp45xLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Apply Learning Rate scheduling\n",
        "\n",
        "import torch.optim.lr_scheduler as lr_scheduler ## Section 8.5.2\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
      ],
      "metadata": {
        "id": "TndTxJWSXxGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block focuses on the training of the GraphSAGE model. It includes the implementation of the training function, which also tracks GPU memory usage at different stages of the training process to monitor the model's performance and resource utilization. The section also involves the testing function to evaluate the model on the validation set.\n"
      ],
      "metadata": {
        "id": "nj2Fy5od-XxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize hyperparameters\n",
        "lr, epochs, hidden_dim = 0.001, 2, 90\n",
        "\n",
        "# Instantiate evaluator for the 'ogbn-products' dataset\n",
        "evaluator = Evaluator(name='ogbn-products')\n",
        "\n",
        "# Instantiate and move the GraphSAGE_dense model to the appropriate device\n",
        "model_base_dense = GraphSAGE_dense(in_dim=data_dense.num_node_features,\n",
        "                                   hidden_dim=hidden_dim,\n",
        "                                   out_dim=dataset_dense.num_classes).to(device)\n",
        "\n",
        "# Initialize Adam optimizer with the model’s parameters\n",
        "optimizer = torch.optim.Adam(model_base_dense.parameters(), lr=lr)\n",
        "\n",
        "# Create a learning rate scheduler to adjust the learning rate based on performance\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1)\n"
      ],
      "metadata": {
        "id": "CTjLzE0z6Fkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pynvml\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from thop import profile\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Initialize NVML library\n",
        "pynvml.nvmlInit()\n",
        "\n",
        "# Function to get the current GPU memory usage\n",
        "def get_gpu_memory_usage():\n",
        "    \"\"\"\n",
        "    Get the current GPU memory usage.\n",
        "\n",
        "    This function retrieves the current memory usage of the GPU by utilizing the NVML library.\n",
        "    The memory usage is returned in megabytes.\n",
        "\n",
        "    Returns:\n",
        "    float: The current GPU memory usage in MB.\n",
        "    \"\"\"\n",
        "\n",
        "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # 0 for the first GPU\n",
        "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "    return info.used / (1024 * 1024)  # Convert bytes to MB\n",
        "\n",
        "# Training function with memory tracking, FLOPs, and epoch time\n",
        "def train_dense(model, batch, optimizer):\n",
        "    \"\"\"\n",
        "    Train a model on a single batch of data, while tracking GPU memory usage, FLOPs, and epoch time.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): The model to be trained.\n",
        "    - batch (torch.Tensor): The batch of data for training.\n",
        "    - optimizer (torch.optim.Optimizer): The optimizer for updating model parameters.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Contains the loss, the number of correctly classified samples, a dictionary\n",
        "           of memory usage after each operation, max memory usage, the operation that\n",
        "           caused max memory usage, and the time taken for the epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    batch = batch.to('cuda')\n",
        "\n",
        "    memory_tracking = {}  # Dictionary to store memory usage after each operation\n",
        "    max_memory = 0        # To store max memory usage\n",
        "    max_memory_step = \"\"  # To store the operation that caused max memory usage\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    memory_tracking['after optimizer.zero_grad()'] = get_gpu_memory_usage()\n",
        "    if memory_tracking['after optimizer.zero_grad()'] > max_memory:\n",
        "        max_memory = memory_tracking['after optimizer.zero_grad()']\n",
        "        max_memory_step = 'after optimizer.zero_grad()'\n",
        "\n",
        "    out = model(batch.x, batch.edge_index)\n",
        "    memory_tracking['after model forward pass'] = get_gpu_memory_usage()\n",
        "    if memory_tracking['after model forward pass'] > max_memory:\n",
        "        max_memory = memory_tracking['after model forward pass']\n",
        "        max_memory_step = 'after model forward pass'\n",
        "\n",
        "    loss = torch.nn.functional.cross_entropy(out, batch.y.squeeze(1).long())\n",
        "    memory_tracking['after loss calculation'] = get_gpu_memory_usage()\n",
        "    if memory_tracking['after loss calculation'] > max_memory:\n",
        "        max_memory = memory_tracking['after loss calculation']\n",
        "        max_memory_step = 'after loss calculation'\n",
        "\n",
        "    loss.backward()\n",
        "    memory_tracking['after loss.backward()'] = get_gpu_memory_usage()\n",
        "    if memory_tracking['after loss.backward()'] > max_memory:\n",
        "        max_memory = memory_tracking['after loss.backward()']\n",
        "        max_memory_step = 'after loss.backward()'\n",
        "\n",
        "    optimizer.step()\n",
        "    memory_tracking['after optimizer.step()'] = get_gpu_memory_usage()\n",
        "    if memory_tracking['after optimizer.step()'] > max_memory:\n",
        "        max_memory = memory_tracking['after optimizer.step()']\n",
        "        max_memory_step = 'after optimizer.step()'\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "\n",
        "\n",
        "    correct = (out.argmax(dim=1) == batch.y.squeeze(1)).sum().item()\n",
        "\n",
        "    # Return the collected data, including memory usage\n",
        "    return loss.item(), correct, memory_tracking, max_memory, max_memory_step, epoch_time\n",
        "\n",
        "\n",
        "# Testing function with memory tracking\n",
        "@torch.no_grad()\n",
        "def test_dense(model, batch):\n",
        "    \"\"\"\n",
        "    Test a model on a single batch of data, while tracking GPU memory usage.\n",
        "\n",
        "    This function evaluates the model's performance and memory usage on a test batch of data.\n",
        "    Memory usage at different steps of the testing process is tracked and returned.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): The model to be tested.\n",
        "    - batch (torch.Tensor): The batch of data for testing.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Contains the predictions, true labels, a dictionary of memory usage after each\n",
        "           operation, max memory usage, and the operation that caused max memory usage.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    batch = batch.to('cuda')\n",
        "\n",
        "    memory_tracking = {}\n",
        "    max_memory = 0\n",
        "    max_memory_step = \"\"\n",
        "\n",
        "    torch.cuda.synchronize()  # Ensure all CUDA operations are completed\n",
        "    out = model(batch.x, batch.edge_index)\n",
        "\n",
        "    torch.cuda.synchronize()  # Ensure all CUDA operations are completed\n",
        "    memory_tracking['after model forward pass'] = get_gpu_memory_usage()\n",
        "    if memory_tracking['after model forward pass'] > max_memory:\n",
        "        max_memory = memory_tracking['after model forward pass']\n",
        "        max_memory_step = 'after model forward pass'\n",
        "\n",
        "    pred = out.argmax(dim=1)\n",
        "\n",
        "    torch.cuda.synchronize()  # Ensure all CUDA operations are completed\n",
        "    memory_tracking['after prediction'] = get_gpu_memory_usage()\n",
        "    if memory_tracking['after prediction'] > max_memory:\n",
        "        max_memory = memory_tracking['after prediction']\n",
        "        max_memory_step = 'after prediction'\n",
        "\n",
        "    # print(memory_tracking)  # Print raw memory values for debugging\n",
        "\n",
        "    return pred.cpu(), batch.y.cpu(), memory_tracking, max_memory, max_memory_step\n",
        "\n"
      ],
      "metadata": {
        "id": "wYiZpvlMoSFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The code is a training loop for training a deep learning model for a total of 100 epochs.\n",
        "\n",
        "2.  For each epoch:\n",
        "\n",
        "-    It trains the model on batches of data obtained from a data loader (loader).\n",
        "-    Tracks GPU memory usage and epoch time during training.\n",
        "-    Computes the average and maximum training memory usage.\n",
        "-    Computes the total time taken for each epoch.\n",
        "-    Collects loss values for each batch.\n",
        "Computes the average loss over all batches.\n",
        "3.  After training in each epoch:\n",
        "\n",
        "-    It evaluates the model on a validation set (valid_loader), tracking the GPU memory usage.\n",
        "-    Computes the average and maximum memory usage during testing.\n",
        "-    Collects all predictions and true labels.\n",
        "\n",
        "4.  It then computes and prints the confusion matrix and classification report for the validation set, offering insights into the model's performance.\n",
        "\n",
        "5. Finally, it prints a summary including:\n",
        "\n",
        "-    The total time taken for the epoch.\n",
        "-    The average and maximum memory usage during training and testing.\n",
        "-    The average loss during training.\n",
        "\n",
        "6.  This loop repeats for each epoch, providing detailed insights into the training process, memory consumption, and model performance on the validation set."
      ],
      "metadata": {
        "id": "lmsRIKCsjk_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Epoch loop\n",
        "epochs = 100\n",
        "for epoch in range(1, epochs + 1):\n",
        "    total_epoch_time = 0\n",
        "    # total_flops = 0\n",
        "    train_memory = []\n",
        "    max_train_memory = 0\n",
        "    loss_values = []\n",
        "\n",
        "    for batch in loader:  # assuming loader is your DataLoader object\n",
        "        loss, _, memory_tracking, max_memory, _, epoch_time = train_dense(model_base_dense, batch, optimizer)\n",
        "        train_memory.append(sum(memory_tracking.values()) / len(memory_tracking))\n",
        "        max_train_memory = max(max_train_memory, max(memory_tracking.values()))\n",
        "        total_epoch_time += epoch_time\n",
        "        # total_flops += int(flops)\n",
        "        loss_values.append(loss)\n",
        "\n",
        "    avg_train_memory = sum(train_memory) / len(train_memory) if train_memory else 0\n",
        "    avg_loss = sum(loss_values) / len(loss_values) if loss_values else 0\n",
        "\n",
        "    test_memory = []\n",
        "    max_test_memory = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "\n",
        "    for batch in valid_loader:  # assuming valid_loader is your DataLoader object for validation set\n",
        "        preds, labels, memory_tracking, max_memory, _ = test_dense(model_base_dense, batch)\n",
        "        test_memory.append(sum(memory_tracking.values()) / len(memory_tracking))\n",
        "        max_test_memory = max(max_test_memory, max(memory_tracking.values()))\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    avg_test_memory = sum(test_memory) / len(test_memory) if test_memory else 0\n",
        "\n",
        "    # Compute the confusion matrix and classification report\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    cr = classification_report(all_labels, all_preds)\n",
        "\n",
        "    print('Confusion Matrix:\\n', cm)\n",
        "    print('Classification Report:\\n', cr)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch} Summary:\")\n",
        "    print(f\"Total Epoch Time: {total_epoch_time:.2f} seconds\")\n",
        "    # print(f\"Total FLOPs: {total_flops}\")\n",
        "    print(f\"Avg Train Memory Usage: {avg_train_memory:.2f} MB\")\n",
        "    print(f\"Max Train Memory Usage: {max_train_memory:.2f} MB\")\n",
        "    print(f\"Avg Test Memory Usage: {avg_test_memory:.2f} MB\")\n",
        "    print(f\"Max Test Memory Usage: {max_test_memory:.2f} MB\")\n",
        "    print(f\"Avg Loss: {avg_loss:.4f}\")\n",
        "    print(f\"{'='*50}\")\n"
      ],
      "metadata": {
        "id": "pOCWbzo8hsch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d0e11lZpe7xi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}