{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 8 Learning at Scale - Section 8.8 - Graph Coarsening\n",
        "\n",
        "-------------------------------------------------------------------\n",
        "\n",
        "This script demonstrates the process of graph coarsening using the Graclus\n",
        "method with PyTorch Geometric. It starts with loading the KarateClub dataset,\n",
        "a well-known social network dataset representing the relationships between\n",
        "34 members of a karate club at a US university in the 1970s.\n",
        "\n",
        "The goal is to perform graph coarsening to reduce the graph's complexity while\n",
        "maintaining its essential structural and feature properties. The Graclus\n",
        "method is used for clustering, followed by max-pooling to create a coarsened\n",
        "graph. Node labels are also updated to reflect the most common label within\n",
        "each cluster."
      ],
      "metadata": {
        "id": "4GGiVabURk0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our process below:\n",
        "1. The KarateClub dataset is loaded.\n",
        "2. Graclus clustering is performed on the graph.\n",
        "3. New labels are generated for the coarsened graph.\n",
        "4. An original training mask is assumed; if it doesn't exist, all nodes are considered for training.\n",
        "5. A new training mask for the coarsened graph is created based on whether the clusters contain any training nodes from the original graph.\n",
        "6. The new training mask is added to the coarsened data object.\n",
        "7. The coarsened data object, with the new training mask, is printed for verification."
      ],
      "metadata": {
        "id": "t8BsTUok_ZOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I. Install Packages, Load Data and Import Packages"
      ],
      "metadata": {
        "id": "U5bh5DQ4V6oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the CUDA version PyTorch was installed with\n",
        "!python -c \"import torch; print(torch.version.cuda)\""
      ],
      "metadata": {
        "id": "F6yxbxHdJSAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch version\n",
        "!python -c \"import torch; print(torch.__version__)\""
      ],
      "metadata": {
        "id": "9MYYJ9nnJThR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the above information to fill in the http address below\n",
        "# %%capture\n",
        "!pip install ogb pyg-lib torch-scatter torch-cluster torch-sparse -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "DMViVQi5DToJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.nn import graclus, max_pool\n",
        "from torch_geometric.utils import to_undirected\n",
        "from torch_geometric.datasets import KarateClub\n",
        "from scipy import stats\n"
      ],
      "metadata": {
        "id": "eWoV5j8M7juL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Coarsen the karate club dataset"
      ],
      "metadata": {
        "id": "SlvY-aWJs4pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load dataset\n",
        "dataset = KarateClub()\n",
        "data = dataset[0]\n",
        "\n",
        "# Print the original labels\n",
        "print(\"Old Labels:\")\n",
        "print(data.y)\n",
        "\n",
        "# Ensure edge_index is undirected for Graclus clustering\n",
        "data.edge_index = to_undirected(data.edge_index)\n",
        "\n",
        "\"\"\"\n",
        "Perform clustering using the Graclus method. The Graclus algorithm is a\n",
        "fast graph clustering method that maximizes the modularity over possible\n",
        "clusterings. This section calculates clusters and prints them.\n",
        "\"\"\"\n",
        "cluster = graclus(data.edge_index, num_nodes=data.num_nodes)\n",
        "cluster_n = cluster.to('cpu').numpy()\n",
        "y_n = data.y.to('cpu').numpy()\n",
        "\n",
        "\"\"\"\n",
        "Generate new labels for the coarsened graph. For each cluster generated\n",
        "by Graclus, find the most common label among the nodes in the cluster\n",
        "and assign it as the new label for the coarsened node corresponding to\n",
        "that cluster.\n",
        "\"\"\"\n",
        "new_labels = []\n",
        "for i in range(cluster_n.max() + 1):\n",
        "    labels_in_cluster = y_n[cluster_n == i]\n",
        "    if labels_in_cluster.size > 0:\n",
        "        most_common_label = stats.mode(labels_in_cluster).mode.item()\n",
        "        new_labels.append(most_common_label)\n",
        "\n",
        "# Convert new_labels to a tensor\n",
        "new_labels = torch.tensor(new_labels, dtype=torch.long)\n",
        "\n",
        "# Print the new labels\n",
        "print(\"\\nNew Labels:\")\n",
        "print(new_labels)\n",
        "\n",
        "# Print the old data object\n",
        "print(\"\\nOld Data Object:\")\n",
        "print(data)\n",
        "\n",
        "\"\"\"\n",
        "Perform max pooling to obtain the coarsened graph, using the clusters\n",
        "obtained from the Graclus algorithm. Max pooling helps to reduce the\n",
        "graph size while retaining the essential structural and feature\n",
        "characteristics.\n",
        "\"\"\"\n",
        "data_coarse = max_pool(cluster, data)\n",
        "data_coarse.y = new_labels\n",
        "\n",
        "# Assuming an original training mask exists; if not, consider all nodes for training\n",
        "original_train_mask = data.train_mask if hasattr(data, 'train_mask') else torch.ones(data.num_nodes, dtype=bool)\n",
        "\n",
        "# Print the original training mask\n",
        "print(\"\\nOriginal Training Mask:\")\n",
        "print(original_train_mask)\n",
        "\n",
        "# Determine whether clusters contain any training nodes from the original graph\n",
        "contains_training_node = torch.zeros(cluster_n.max() + 1, dtype=bool)\n",
        "for i in range(cluster_n.max() + 1):\n",
        "    nodes_in_cluster = cluster_n == i\n",
        "    if original_train_mask[nodes_in_cluster].any().item():\n",
        "        contains_training_node[i] = True\n",
        "\n",
        "# Create a new training mask for the coarsened graph\n",
        "new_train_mask = contains_training_node[new_labels.long()]\n",
        "\n",
        "# Print the new training mask\n",
        "print(\"\\nNew Training Mask:\")\n",
        "print(new_train_mask)\n",
        "\n",
        "# Add the new training mask to the coarsened data object\n",
        "data_coarse.train_mask = new_train_mask\n",
        "\n",
        "# Print the new coarsened data object with the training mask\n",
        "print(\"\\nNew Data Object with Training Mask:\")\n",
        "print(data_coarse)\n"
      ],
      "metadata": {
        "id": "8eosOX4n7rO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Coarsen the Amazon products dataset"
      ],
      "metadata": {
        "id": "na0YIUFJs889"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing obg datatset\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import torch_geometric.transforms as T"
      ],
      "metadata": {
        "id": "njh8h9tb-zU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "root = osp.join(osp.dirname(osp.realpath('./')), 'data', 'products')\n"
      ],
      "metadata": {
        "id": "NFifNUQX5LMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dense = PygNodePropPredDataset( name='ogbn-products')\n",
        "dataset_dense.num_classes"
      ],
      "metadata": {
        "id": "6NqOhSmf5N8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Load dataset\n",
        "dataset = dataset_dense\n",
        "data = dataset[0]\n",
        "\n",
        "# Print the original labels\n",
        "print(\"Old Labels:\")\n",
        "print(data.y)\n",
        "\n",
        "# Ensure edge_index is undirected for Graclus clustering\n",
        "data.edge_index = to_undirected(data.edge_index)\n",
        "\n",
        "\"\"\"\n",
        "Perform clustering using the Graclus method. The Graclus algorithm is a\n",
        "fast graph clustering method that maximizes the modularity over possible\n",
        "clusterings. This section calculates clusters and prints them.\n",
        "\"\"\"\n",
        "cluster = graclus(data.edge_index, num_nodes=data.num_nodes)\n",
        "cluster_n = cluster.to('cpu').numpy()\n",
        "y_n = data.y.to('cpu').numpy()\n",
        "\n",
        "\"\"\"\n",
        "Generate new labels for the coarsened graph. For each cluster generated\n",
        "by Graclus, find the most common label among the nodes in the cluster\n",
        "and assign it as the new label for the coarsened node corresponding to\n",
        "that cluster.\n",
        "\"\"\"\n",
        "new_labels = []\n",
        "for i in range(cluster_n.max() + 1):\n",
        "    labels_in_cluster = y_n[cluster_n == i]\n",
        "    if labels_in_cluster.size > 0:\n",
        "        most_common_label = stats.mode(labels_in_cluster).mode.item()\n",
        "        new_labels.append(most_common_label)\n",
        "\n",
        "# Convert new_labels to a tensor\n",
        "new_labels = torch.tensor(new_labels, dtype=torch.long)\n",
        "\n",
        "# Print the new labels\n",
        "print(\"\\nNew Labels:\")\n",
        "print(new_labels)\n",
        "\n",
        "# Print the old data object\n",
        "print(\"\\nOld Data Object:\")\n",
        "print(data)\n",
        "\n",
        "\"\"\"\n",
        "Perform max pooling to obtain the coarsened graph, using the clusters\n",
        "obtained from the Graclus algorithm. Max pooling helps to reduce the\n",
        "graph size while retaining the essential structural and feature\n",
        "characteristics.\n",
        "\"\"\"\n",
        "data_coarse = max_pool(cluster, data)\n",
        "data_coarse.y = new_labels\n",
        "\n",
        "# Assuming an original training mask exists; if not, consider all nodes for training\n",
        "original_train_mask = data.train_mask if hasattr(data, 'train_mask') else torch.ones(data.num_nodes, dtype=bool)\n",
        "\n",
        "# Print the original training mask\n",
        "print(\"\\nOriginal Training Mask:\")\n",
        "print(original_train_mask)\n",
        "\n",
        "# Determine whether clusters contain any training nodes from the original graph\n",
        "contains_training_node = torch.zeros(cluster_n.max() + 1, dtype=bool)\n",
        "for i in range(cluster_n.max() + 1):\n",
        "    nodes_in_cluster = cluster_n == i\n",
        "    if original_train_mask[nodes_in_cluster].any().item():\n",
        "        contains_training_node[i] = True\n",
        "\n",
        "# Create a new training mask for the coarsened graph\n",
        "new_train_mask = contains_training_node[new_labels.long()]\n",
        "\n",
        "# Print the new training mask\n",
        "print(\"\\nNew Training Mask:\")\n",
        "print(new_train_mask)\n",
        "\n",
        "# Add the new training mask to the coarsened data object\n",
        "data_coarse.train_mask = new_train_mask\n",
        "\n",
        "# Print the new coarsened data object with the training mask\n",
        "print(\"\\nNew Data Object with Training Mask:\")\n",
        "print(data_coarse)\n"
      ],
      "metadata": {
        "id": "ZuR-Za6z5Uoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "36fj0RV-7LTO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
