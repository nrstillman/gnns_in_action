{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Section 8.8 of GNNs in Action\n## Parallel & Distributed Processing\n\nThis notebook demonstrates the use of the datadistributedparallel class in training a GNN model on a machine with two GPUs. \n\nNOTE: You must activate the  GPU T4x2 accelerator to use this code.\n\nNOTE2: Make sure the internet is switched to 'Internet on'.","metadata":{"_uuid":"fce707d7-9d00-4884-bcb7-6b7803a94010","_cell_guid":"6b66cd97-badd-405c-ad64-df71d9e6ba9b","trusted":true}},{"cell_type":"markdown","source":"In this notebook, we’re dealing with a delicate balance between the versions of PyTorch, CUDA, and the *ogb* and *datadistributedparallel* modules.\n\nWe use !ls -l /usr/local | grep cuda to check the CUDA versions installed. \n\nNext, we uninstall PyTorch and reinstall a specific version (2.0.1+cu118). This version works with the current CUDA version, and the *ogb* and *datadistributedparallel* modules.\n\nBut be aware, the CUDA version has changed before and caused this notebook to fail. If you get errors or the notebook breaks, it’s likely due to a mismatch between PyTorch, CUDA, and PyG.","metadata":{}},{"cell_type":"code","source":"# Find the CUDA version PyTorch was installed with\n!python -c \"import torch; print(torch.version.cuda)\"","metadata":{"execution":{"iopub.status.busy":"2023-10-25T05:47:30.116943Z","iopub.execute_input":"2023-10-25T05:47:30.117844Z","iopub.status.idle":"2023-10-25T05:47:38.569473Z","shell.execute_reply.started":"2023-10-25T05:47:30.117810Z","shell.execute_reply":"2023-10-25T05:47:38.568514Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"11.8\n","output_type":"stream"}]},{"cell_type":"code","source":"# PyTorch version\n!python -c \"import torch; print(torch.__version__)\"","metadata":{"execution":{"iopub.status.busy":"2023-10-25T05:47:38.572046Z","iopub.execute_input":"2023-10-25T05:47:38.572432Z","iopub.status.idle":"2023-10-25T05:47:41.471713Z","shell.execute_reply.started":"2023-10-25T05:47:38.572392Z","shell.execute_reply":"2023-10-25T05:47:41.470626Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls -l /usr/local | grep cuda","metadata":{"execution":{"iopub.status.busy":"2023-10-25T05:47:41.473226Z","iopub.execute_input":"2023-10-25T05:47:41.473600Z","iopub.status.idle":"2023-10-25T05:47:42.458014Z","shell.execute_reply.started":"2023-10-25T05:47:41.473562Z","shell.execute_reply":"2023-10-25T05:47:42.456927Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"lrwxrwxrwx 1 root root   22 Jun 21 00:40 cuda -> /etc/alternatives/cuda\nlrwxrwxrwx 1 root root   25 Jun 21 00:40 cuda-11 -> /etc/alternatives/cuda-11\ndrwxr-xr-x 1 root root 4096 Jun 21 01:01 cuda-11.8\nlrwxrwxrwx 1 root root   25 Jun 26 04:50 cuda-12 -> /etc/alternatives/cuda-12\ndrwxr-xr-x 3 root root 4096 Jun 26 04:49 cuda-12.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2023-10-25T05:47:42.460884Z","iopub.execute_input":"2023-10-25T05:47:42.461250Z","iopub.status.idle":"2023-10-25T05:47:43.457700Z","shell.execute_reply.started":"2023-10-25T05:47:42.461221Z","shell.execute_reply":"2023-10-25T05:47:43.456619Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip uninstall torch -y","metadata":{"_uuid":"316df8e1-3a6e-40dd-9ce2-8aee15e0cb88","_cell_guid":"45a6826f-c37a-4c7f-9295-d99a098083f5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-25T05:47:43.459186Z","iopub.execute_input":"2023-10-25T05:47:43.459545Z","iopub.status.idle":"2023-10-25T05:48:12.684658Z","shell.execute_reply.started":"2023-10-25T05:47:43.459513Z","shell.execute_reply":"2023-10-25T05:48:12.683600Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.0.0\nUninstalling torch-2.0.0:\n  Successfully uninstalled torch-2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torch==2.0.1 --index-url https://download.pytorch.org/whl/cu118","metadata":{"_uuid":"d3e2c4d2-39ff-4e51-a320-65371371adb6","_cell_guid":"6ebfd462-c8d5-46c9-918f-3a88d19c77d8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-25T05:48:12.686045Z","iopub.execute_input":"2023-10-25T05:48:12.686342Z","iopub.status.idle":"2023-10-25T05:50:10.774040Z","shell.execute_reply.started":"2023-10-25T05:48:12.686314Z","shell.execute_reply":"2023-10-25T05:50:10.772826Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nCollecting torch==2.0.1\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.1.2)\nCollecting triton==2.0.0 (from torch==2.0.1)\n  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting cmake (from triton==2.0.0->torch==2.0.1)\n  Downloading https://download.pytorch.org/whl/cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lit (from triton==2.0.0->torch==2.0.1)\n  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1) (1.3.0)\nBuilding wheels for collected packages: lit\n  Building wheel for lit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89988 sha256=48a99b7cd9456ea981ad69c16e7e257ec5977210775bc22703c0334a094cf3a2\n  Stored in directory: /root/.cache/pip/wheels/27/2c/b6/3ed2983b1b44fe0dea1bb35234b09f2c22fb8ebb308679c922\nSuccessfully built lit\nInstalling collected packages: lit, cmake, triton, torch\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cmake-3.25.0 lit-15.0.7 torch-2.0.1+cu118 triton-2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"print('test')","metadata":{"_uuid":"ce53af6f-f780-4f75-b1a8-c26bbaab0ec2","_cell_guid":"99c8489e-6319-4e58-906a-787c5597d773","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-25T05:50:10.775689Z","iopub.execute_input":"2023-10-25T05:50:10.776023Z","iopub.status.idle":"2023-10-25T05:50:10.781189Z","shell.execute_reply.started":"2023-10-25T05:50:10.775981Z","shell.execute_reply":"2023-10-25T05:50:10.780313Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"test\n","output_type":"stream"}]},{"cell_type":"code","source":"# Find the CUDA version PyTorch was installed with\n!python -c \"import torch; print(torch.version.cuda)\"","metadata":{"_uuid":"a4b1a3fc-7b7b-418c-be1b-1aeff219b11d","_cell_guid":"c1be376c-c7af-4231-85c2-6954d5a83139","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-25T05:50:10.782255Z","iopub.execute_input":"2023-10-25T05:50:10.782538Z","iopub.status.idle":"2023-10-25T05:50:13.872619Z","shell.execute_reply.started":"2023-10-25T05:50:10.782513Z","shell.execute_reply":"2023-10-25T05:50:13.871574Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"11.8\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2023-10-25T05:50:13.874306Z","iopub.execute_input":"2023-10-25T05:50:13.874756Z","iopub.status.idle":"2023-10-25T05:50:14.861961Z","shell.execute_reply.started":"2023-10-25T05:50:13.874710Z","shell.execute_reply":"2023-10-25T05:50:14.861018Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\n","output_type":"stream"}]},{"cell_type":"code","source":"! ls -l /usr/local | grep cuda","metadata":{"execution":{"iopub.status.busy":"2023-10-25T05:50:14.865155Z","iopub.execute_input":"2023-10-25T05:50:14.865509Z","iopub.status.idle":"2023-10-25T05:50:15.820013Z","shell.execute_reply.started":"2023-10-25T05:50:14.865475Z","shell.execute_reply":"2023-10-25T05:50:15.819065Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"lrwxrwxrwx 1 root root   22 Jun 21 00:40 cuda -> /etc/alternatives/cuda\nlrwxrwxrwx 1 root root   25 Jun 21 00:40 cuda-11 -> /etc/alternatives/cuda-11\ndrwxr-xr-x 1 root root 4096 Jun 21 01:01 cuda-11.8\nlrwxrwxrwx 1 root root   25 Jun 26 04:50 cuda-12 -> /etc/alternatives/cuda-12\ndrwxr-xr-x 3 root root 4096 Jun 26 04:49 cuda-12.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# PyTorch version\n!python -c \"import torch; print(torch.__version__)\"","metadata":{"_uuid":"06328210-b6cf-4fba-9132-3998e349ef80","_cell_guid":"05af72f1-1785-4959-a1d5-245ffbfd8c6a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-25T05:50:15.821291Z","iopub.execute_input":"2023-10-25T05:50:15.821579Z","iopub.status.idle":"2023-10-25T05:50:18.877354Z","shell.execute_reply.started":"2023-10-25T05:50:15.821551Z","shell.execute_reply":"2023-10-25T05:50:18.876340Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"2.0.1+cu118\n","output_type":"stream"}]},{"cell_type":"code","source":"#\n\n!pip install torch_geometric\n\n# Optional dependencies (note the versions of pytorch and cuda on the wheel URL \n# match with what we installed above.):\n\n!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n","metadata":{"_uuid":"e167a01d-3d52-48a8-8df3-8b6a76742d41","_cell_guid":"4de72ae5-a0df-4d18-a341-3ab3297630bf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-25T05:50:18.878887Z","iopub.execute_input":"2023-10-25T05:50:18.879216Z","iopub.status.idle":"2023-10-25T05:50:48.706692Z","shell.execute_reply.started":"2023-10-25T05:50:18.879185Z","shell.execute_reply":"2023-10-25T05:50:48.705680Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.23.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.11.2)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.31.0)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2023.7.22)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.1.0)\nInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.4.0\nLooking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\nCollecting pyg_lib\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/pyg_lib-0.3.0%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torch_scatter\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting torch_sparse\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.18%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting torch_cluster\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.3%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting torch_spline_conv\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (886 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m886.6/886.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_sparse) (1.11.2)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy->torch_sparse) (1.23.5)\nInstalling collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\nSuccessfully installed pyg_lib-0.3.0+pt20cu118 torch_cluster-1.6.3+pt20cu118 torch_scatter-2.1.2+pt20cu118 torch_sparse-0.6.18+pt20cu118 torch_spline_conv-1.2.2+pt20cu118\n","output_type":"stream"}]},{"cell_type":"code","source":"####\n!pip install ogb\n!pip install gputil\n!pip install nvidia-ml-py3\n!pip install thop","metadata":{"_uuid":"36e47e28-ab36-4789-aad9-3fe2da5f5a06","_cell_guid":"8b519504-91f2-4969-be9a-ba67256d7397","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-25T05:50:48.708364Z","iopub.execute_input":"2023-10-25T05:50:48.708821Z","iopub.status.idle":"2023-10-25T05:51:43.083945Z","shell.execute_reply.started":"2023-10-25T05:50:48.708773Z","shell.execute_reply":"2023-10-25T05:51:43.082800Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Collecting ogb\n  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (2.0.1+cu118)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.23.5)\nRequirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (4.66.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.2.2)\nRequirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (2.0.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.16.0)\nRequirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.26.15)\nCollecting outdated>=0.2.0 (from ogb)\n  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\nRequirement already satisfied: setuptools>=44 in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (68.0.0)\nCollecting littleutils (from outdated>=0.2.0->ogb)\n  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (2.31.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2023.3)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (1.11.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.1.2)\nRequirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (2.0.0)\nRequirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.25.0)\nRequirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->ogb) (15.0.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\nBuilding wheels for collected packages: littleutils\n  Building wheel for littleutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=82c768e373afd10652304b8422c6422e9b576bd33500a51ec70b40e0f52e92bf\n  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\nSuccessfully built littleutils\nInstalling collected packages: littleutils, outdated, ogb\nSuccessfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\nCollecting gputil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: gputil\n  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7393 sha256=eda1a62a99270bdf2093ccb74853cf524bc851ab9f5dedf37bb3d398e1d80408\n  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\nSuccessfully built gputil\nInstalling collected packages: gputil\nSuccessfully installed gputil-1.4.0\nCollecting nvidia-ml-py3\n  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: nvidia-ml-py3\n  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19171 sha256=231155db0c72471b77c30320ffde44d2c10067b6adce960d8243583512a0e26d\n  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\nSuccessfully built nvidia-ml-py3\nInstalling collected packages: nvidia-ml-py3\nSuccessfully installed nvidia-ml-py3-7.352.0\nCollecting thop\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from thop) (2.0.1+cu118)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->thop) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->thop) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.1.2)\nRequirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (2.0.0)\nRequirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch->thop) (3.25.0)\nRequirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch->thop) (15.0.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->thop) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->thop) (1.3.0)\nInstalling collected packages: thop\nSuccessfully installed thop-0.1.1.post2209072238\n","output_type":"stream"}]},{"cell_type":"code","source":"# Note: If you encounter some strange error, just run this cell again.\nimport torch\nfrom torch_geometric.data import Data, DataLoader\nfrom ogb.nodeproppred import PygNodePropPredDataset\ndataset = PygNodePropPredDataset(name='ogbn-products')","metadata":{"_uuid":"8eaff2f3-3ba5-4e58-a33e-1a28e03964f3","_cell_guid":"680d0430-31b7-4309-b286-e65227b44227","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-25T05:51:43.085442Z","iopub.execute_input":"2023-10-25T05:51:43.085770Z","iopub.status.idle":"2023-10-25T05:55:21.349682Z","shell.execute_reply.started":"2023-10-25T05:51:43.085739Z","shell.execute_reply":"2023-10-25T05:55:21.348445Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"This will download 1.38GB. Will you proceed? (y/N)\n y\n"},{"name":"stdout","text":"Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n","output_type":"stream"},{"name":"stderr","text":"Downloaded 1.38 GB: 100%|██████████| 1414/1414 [01:43<00:00, 13.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/products.zip\n","output_type":"stream"},{"name":"stderr","text":"Processing...\n","output_type":"stream"},{"name":"stdout","text":"Loading necessary files...\nThis might take a while.\nProcessing graphs...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"Converting graphs into PyG objects...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 2366.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saving...\n","output_type":"stream"},{"name":"stderr","text":"Done!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Encapsulate model and training into a file\nWe're writing the GCN model and training function into a separate Python file 'my_module.py'. This separation is essential for multiprocessing, particularly when using the multiprocessing library in PyTorch. By placing the model and training function in a separate file, we ensure that each process can import and access these components cleanly, preventing potential issues related to variable scope, function definitions, and Python's \"__main__\" guard during multiprocessing.\n\nAs for the script itself, this Python script is designed for distributed training of a Graph Convolutional Network (GCN) on a graph-based dataset using PyTorch and PyTorch Geometric. The code is divided into several parts, each serving a specific purpose. First, necessary libraries and modules are imported, including those required for distributed computing and memory usage tracking. The get_memory_usage function uses the psutil library to monitor the memory usage of the running process. The GCN class defines the architecture of the Graph Convolutional Network, including two GCN layers and a final fully connected layer for classification.\n\nThe train function is where the model is trained for one epoch. It calculates the loss, performs backpropagation, and updates the model’s weights. During each epoch, it monitors and prints the memory usage and epoch time. The main function sets up distributed training, where each process is assigned to a separate GPU. It initializes the model, data, and other training necessities and runs the training loop for a specified number of epochs. It calculates and prints the average epoch time, memory usage, and total convergence time for the training process.\n","metadata":{}},{"cell_type":"code","source":"%%writefile my_module.py\n\n# Importing necessary libraries\nimport torch\nfrom torch.nn import Linear\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.loader import NeighborLoader\nfrom ogb.nodeproppred import PygNodePropPredDataset\nimport time\nimport os\nimport psutil  # Library for retrieving system-level information, for our case, CPU memory usage\nimport GPUtil\nimport pynvml\nimport numpy as np  \nimport logging  \nfrom thop import profile  \n\n# Suppress INFO messages from thop\nlogger = logging.getLogger('thop')\nlogger.setLevel(logging.ERROR)\nlogging.getLogger('thop').setLevel(logging.WARNING)  \n\n# Initialize NVML library\npynvml.nvmlInit()\n\n# Function to get the current process's memory usage\ndef get_cpu_memory_usage():\n    \"\"\"\n    Returns the current memory usage of the cpu process.\n\n    :return: Memory usage in bytes\n    \"\"\"\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss\n\n\ndef get_gpu_memory_usage():\n    \"\"\"\n    Returns the current GPU memory usage using pynvml.\n\n    :return: GPU Memory usage in bytes\n    \"\"\"\n    handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # 0 for the first GPU\n    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n    return info.used  # This will return the used GPU memory in bytes\n\n\n\n# GCN (Graph Convolutional Network) model definition\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(100, 128)  # First GCN layer\n        self.conv2 = GCNConv(128, 128)  # Second GCN layer\n        self.fc = Linear(128, 47)  # Final fully connected layer, 47 classes for ogbn-products dataset\n\n    def forward(self, x, edge_index):\n        \"\"\"\n        Forward pass through the network.\n\n        :param x: Input features\n        :param edge_index: Edge indices defining the graph structure\n        :return: Output after passing through network\n        \"\"\"\n        x = self.conv1(x, edge_index)\n        x = self.conv2(x, edge_index)\n        x = self.fc(x)\n        return x\n\n# Function for training the model\ndef train(model, trainloader, criterion, optimizer, device, epoch):\n    model.train()\n    total_loss = 0.0\n    num_batches = 0\n\n    start_time = time.time()\n\n    memory_tracking = {\n        key: [] for key in [\n            'optimizer.zero_grad', 'model forward pass', 'target processing',\n            'loss calculation', 'loss.backward', 'optimizer.step'\n        ]\n    }\n\n    max_memory = 0  # Initialize the max_memory here\n    max_memory_step = \"\"\n    \n    total_flops = 0  # Initialize total FLOPs here\n\n    for batch in trainloader:\n        optimizer.zero_grad()\n        memory = get_gpu_memory_usage()\n        memory_tracking['optimizer.zero_grad'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'optimizer.zero_grad'\n\n        out = model(batch.x.float(), batch.edge_index)\n        memory = get_gpu_memory_usage()\n        memory_tracking['model forward pass'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'model forward pass'\n\n        target = batch.y.view(-1)\n        memory = get_gpu_memory_usage()\n        memory_tracking['target processing'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'target processing'\n\n        loss = criterion(out, target)\n        memory = get_gpu_memory_usage()\n        memory_tracking['loss calculation'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'loss calculation'\n\n        loss.backward()\n        memory = get_gpu_memory_usage()\n        memory_tracking['loss.backward'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'loss.backward'\n\n        optimizer.step()\n        memory = get_gpu_memory_usage()\n        memory_tracking['optimizer.step'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'optimizer.step'\n\n        # Calculating FLOPs\n        with torch.no_grad():\n            macs, params = profile(model.module, inputs=(batch.x.float().to(device), batch.edge_index.to(device)), verbose=False)\n            total_flops += macs * 2  # Convert MACs to FLOPs\n        \n        total_loss += loss.item()\n        num_batches += 1\n\n    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n\n    # Calculate and print memory stats\n    avg_memories = {k: np.mean(v) if v else 0 for k, v in memory_tracking.items()}\n    overall_avg_memory = np.mean([mem for mem in avg_memories.values() if mem > 0])\n\n    end_time = time.time()\n    epoch_time = end_time - start_time\n\n    # Printing epoch time, loss, memory, and FLOPs stats\n#     print(f\"Device: {device}, Epoch: {epoch + 1}, Avg Loss: {avg_loss}, \"\n#           f\"Avg Memory Usage: {overall_avg_memory/(1024 * 1024)}MB, \"\n#           f\"Max Memory Usage: {max_memory/(1024 * 1024)}MB at {max_memory_step}, \"\n#           f\"FLOPs: {total_flops/num_batches if num_batches > 0 else 0}, \"  # Printing average FLOPs per batch\n#           f\"Epoch Time: {epoch_time}s\\n\")\n    print(f\"Device: {device}\\n\"\n      f\"Epoch: {epoch + 1}\\n\"\n      f\"Avg Loss: {avg_loss:.4f}\\n\"\n      f\"Avg Memory Usage: {overall_avg_memory/(1024 * 1024):.2f} MB\\n\"\n      f\"Max Memory Usage: {max_memory/(1024 * 1024):.2f} MB at {max_memory_step}\\n\"\n      f\"FLOPs: {(total_flops/num_batches if num_batches > 0 else 0):.2f}\\n\"\n      f\"Epoch Time: {epoch_time:.2f} s\\n\"\n      f\"{'='*40}\")\n\n    return epoch_time, overall_avg_memory, max_memory\n\n\n# Main function for distributed training\ndef main(rank, world_size, dataset):\n    \"\"\"\n    Main training loop for distributed training.\n\n    :param rank: Rank of the current process\n    :param world_size: Total number of processes\n    :param dataset: Dataset for training\n    \"\"\"\n    convergence_start_time = time.time()  # Time at the start of training\n    \n    split_idx = dataset.get_idx_split()\n    train_idx = split_idx[\"train\"]\n    \n    # Initializing distributed process group\n    dist.init_process_group(\n        backend='nccl',\n        init_method='tcp://localhost:23456',\n        rank=rank,\n        world_size=world_size\n    )\n    device = torch.device(f'cuda:{rank}')  # Set device for the current process\n    \n    data = dataset[0].to(device)  # Load data to the device\n\n    # Initializing DataLoader with neighbor sampling\n    trainloader = NeighborLoader(\n        data,\n        num_neighbors=[15, 10, 5],\n        batch_size=128 * 10,\n        input_nodes=train_idx,\n        shuffle=True,\n        persistent_workers=False\n    )\n  \n    model = GCN().to(device)\n    model = DistributedDataParallel(model, device_ids=[rank])  # Wrap model for distributed training\n\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Using Adam optimizer\n    \n    num_epochs = 10  # Total number of epochs\n    total_epoch_time = 0\n    total_memory_usage = 0\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        epoch_time, avg_memory_usage, max_memory_usage = train(model, trainloader, criterion, optimizer, device, epoch)\n    \n        total_epoch_time += epoch_time\n        total_memory_usage += avg_memory_usage\n    \n    # Calculating and printing average values and total convergence time\n    avg_epoch_time = total_epoch_time / num_epochs\n    avg_memory_usage = total_memory_usage / num_epochs * (1024 *1024)\n    convergence_time = time.time() - convergence_start_time\n    \n    print(f\"Average Epoch Time: {avg_epoch_time}s\")\n    print(f\"Average Memory Usage: {avg_memory_usage}MB\")\n    print(f\"Total Convergence Time: {convergence_time}s\")\n\n    pynvml.nvmlShutdown()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T05:55:21.351515Z","iopub.execute_input":"2023-10-25T05:55:21.351956Z","iopub.status.idle":"2023-10-25T05:55:21.364741Z","shell.execute_reply.started":"2023-10-25T05:55:21.351917Z","shell.execute_reply":"2023-10-25T05:55:21.363740Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Writing my_module.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This script is responsible for initializing and executing the distributed training process. It begins by importing the main function from a file named \"my_module.py\". This main function contains the logic for training a GCN model, monitoring its performance, and tracking resource usage metrics like memory and time.\n\nThe script then imports PyTorch's multiprocessing module to facilitate the use of multiple processors, enabling parallelized operations and speeding up the training process.\n\nInside the if __name__ == '__main__': block, an initial print statement indicates the start of the preprocessing steps. The world_size variable is set to 2, implying that two separate processes will be spawned for training, each potentially utilizing a different GPU or set of computational resources.\n\nThe mp.spawn function is crucial here. It initiates the distributed training by creating world_size number of processes. Each process runs the main function with the provided arguments, performing the training concurrently and ensuring efficient utilization of available resources.\n\n","metadata":{}},{"cell_type":"code","source":"%%time \n\n# (Note: In the module above, I've set the epochs to 10. \n# Of course feel free to adjust as needed.)\n\n \n\n# Importing the 'main' function from 'my_module.py' which contains the \n# entire training process and resource tracking mechanisms.\nfrom my_module import main  \n\n# Importing the multiprocessing module from PyTorch, enabling the ability to \n# use multiple processes for parallel and distributed training.\nimport torch.multiprocessing as mp\n\n# Ensuring the main code is run under the __main__ scope to avoid potential\n# issues with multiprocessing across different modules.\nif __name__ == '__main__':\n    \n    # Setting the 'world_size' variable to 2, indicating that two processes \n    # will be spawned for distributed training.\n    # In the context of multi-GPU training, this typically means \n    # training will utilize two GPUs.\n    \n    world_size = 2  \n    \n    # The 'mp.spawn' function is used to spawn 'world_size' number of \n    # processes that will execute the 'main' function.\n    # Each process will run on a separate GPU (or other resources), \n    # enabling parallel and distributed training.\n    # The 'args' parameter is used to pass arguments to the 'main' \n    # function in each process.\n    # 'nprocs' is set to 'world_size', ensuring the number of processes \n    # spawned equals the defined 'world_size'.\n    # 'join=True' means the main process will wait for all spawned \n    # processes to complete before proceeding.\n    \n    mp.spawn(main, args=(world_size, dataset,), nprocs=world_size, join=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T05:57:41.429126Z","iopub.execute_input":"2023-10-25T05:57:41.429879Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}